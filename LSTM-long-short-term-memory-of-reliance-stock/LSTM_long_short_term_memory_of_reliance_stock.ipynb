{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyqY5csYwdO+x+05aQmpd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmahawar/deep-learning-projects/blob/main/LSTM-long-short-term-memory-of-reliance-stock/LSTM_long_short_term_memory_of_reliance_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryQM-zgJjjr5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load dataset\n",
        "# Replace 'reliance_stock.csv' with your dataset file\n",
        "data = pd.read_csv('https://github.com/vmahawar/data-science-datasets-collection/raw/main/reliance_stock.csv')\n",
        "\n",
        "# Extract closing price and normalize data\n",
        "closing_prices = data['Close'].values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "closing_prices_scaled = scaler.fit_transform(closing_prices.reshape(-1, 1))\n",
        "\n",
        "# Function to prepare data with time window\n",
        "def create_dataset(dataset, time_steps=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset) - time_steps):\n",
        "        X.append(dataset[i:i + time_steps, 0])\n",
        "        y.append(dataset[i + time_steps, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare data for both 30-day and 120-day time windows\n",
        "time_window = 30  # Change to 120 for the second case\n",
        "X, y = create_dataset(closing_prices_scaled, time_window)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM (samples, time_steps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "    LSTM(60, return_sequences=True),\n",
        "    LSTM(20),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss (MSE): {test_loss}\")\n",
        "\n",
        "# Predict and inverse scale predictions\n",
        "predicted_prices = model.predict(X_test)\n",
        "predicted_prices = scaler.inverse_transform(predicted_prices.reshape(-1, 1))"
      ]
    }
  ]
}